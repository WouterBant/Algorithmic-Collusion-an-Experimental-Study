{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for ALGORITHMIC COLLUSION: A COMPUTATIONAL STUDY OF FIRMSâ€™ CSR INVESTMENT DECISIONS (Bant (2023))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BSc Econometrics, University of Amsterdam\n",
    "##### Course: Bachelor's Thesis and Thesis Seminar Econometrics (2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Used for seed for np.random\n",
    "Reproduce = 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse demand- and cost function as in paper, together with resulting profit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the inverse demand for player 1 and 2 in a tuple\n",
    "def P(q1, q2, Theta1, Theta2, Xi=6, Mu=1, Lambda=1):\n",
    "    p1 = Xi - Mu * (q1 + q2) + Lambda * (Theta2 - Theta1)\n",
    "    p2 = Xi - Mu * (q1 + q2) + Lambda * (Theta1 - Theta2)\n",
    "    return (p1, p2)\n",
    "\n",
    "# Return the costs for player 1 and 2 in a tuple\n",
    "def C(Theta1, Theta2, Phi):\n",
    "    c1 = Phi * (1-Theta1)**2\n",
    "    c2 = Phi * (1-Theta2)**2\n",
    "    return (c1, c2)\n",
    "\n",
    "# Return the profit for player 1 and 2 in a tuple\n",
    "def Pi(q1, q2, Theta1, Theta2, Phi, Xi=6, Mu=1, Lambda=1):\n",
    "    p1, p2 = P(q1, q2, Theta1, Theta2, Xi, Mu, Lambda)\n",
    "    c1, c2 = C(Theta1, Theta2, Phi)\n",
    "    pi1, pi2 = q1*p1-c1, q2*p2-c2\n",
    "    return (pi1, pi2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the next action the current player will take (epsilon greedy)\n",
    "# Note: this action is the next state for the other player\n",
    "def get_next_state(Q, state, actions, epsilon):\n",
    "    random_action = np.random.choice(actions)\n",
    "    greedy_action = max(Q[state], key = Q[state].get)\n",
    "    return np.random.choice([random_action, greedy_action], p = [epsilon, 1 - epsilon])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning algorithm corresponding with the pseudocode in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearn(Qs, k=6, k2=4, qc=1.5, qn=2, alpha=6, gamma=0.9, T=500_000, L=1_000):\n",
    "    # Given k1, k2 generate the possible actions\n",
    "    start_q = ((1+k)*qc - qn) / k\n",
    "    step_q = (qn-qc) / k\n",
    "    Thetas = np.array([theta/k2 for theta in range(k2+1)])\n",
    "    Q_mesh, Theta_mesh = np.meshgrid(Qs, Thetas)\n",
    "    actions_h = np.column_stack((Q_mesh.ravel(), Theta_mesh.ravel()))\n",
    "    actions = np.array([tuple(action) for action in actions_h], dtype=[('Q', float), ('Theta', float)])\n",
    "\n",
    "    # Pick the first to prices of both players randomly\n",
    "    Action1, Action2, Action1_next, Action2_next = np.random.choice(actions, size=4)\n",
    "    \n",
    "    # Initialize the Q matrix for both players\n",
    "    Q1 = defaultdict(lambda: dict(zip(actions, np.zeros(len(actions)))))\n",
    "    Q2 = defaultdict(lambda: dict(zip(actions, np.zeros(len(actions)))))\n",
    "    \n",
    "    # Initialize Epsilon\n",
    "    Epsilon = 1\n",
    "\n",
    "    # Keep track of metrics\n",
    "    prof_p1_ep, prof_p2_ep = [], []\n",
    "    q1, q2 = [], []\n",
    "    theta1, theta2 = [], []\n",
    "\n",
    "    for t in range(3, T+1):\n",
    "        # Player 1's turn\n",
    "        # Calculate the current profit\n",
    "        prof_p1, prof_p2 = Pi(Action1_next, Action2)\n",
    "\n",
    "        # Add q and theta\n",
    "        q1.append(Action1_next[0])\n",
    "        theta1.append(Action1_next[1])\n",
    "\n",
    "        # Get the v-value of the next state\n",
    "        v = max(Q1[Action2_next].values())\n",
    "        \n",
    "        # Update Q1 (state X action)\n",
    "        Q1[Action2][Action1_next] += alpha * (prof_p1 + gamma * Pi(Action1_next, Action2_next)[0] + gamma**2 * v - Q1[Action2][Action1_next])\n",
    "\n",
    "        # Update current action player 1, and determine next action based on the next state\n",
    "        Action1, Action1_next = Action1_next, get_next_state(Q1, Action2_next, actions, Epsilon)\n",
    "\n",
    "        # Append the profits to the lists of profits\n",
    "        prof_p1_ep.append(prof_p1)\n",
    "        prof_p2_ep.append(prof_p2)\n",
    "\n",
    "        # Player 2's turn\n",
    "        # Calculate the current profit\n",
    "        prof_p1, prof_p2 = Pi(Action1, Action2_next)\n",
    "\n",
    "        # Add market price to list\n",
    "        # Add q and theta\n",
    "        q2.append(Action2_next[0])\n",
    "        theta2.append(Action2_next[1])\n",
    "\n",
    "        # Get the v-value of the next state\n",
    "        v = max(Q2[Action1_next].values())\n",
    "        \n",
    "        # Update Q2 (state X action)\n",
    "        Q2[Action1][Action2_next] += alpha * (prof_p2 + gamma * Pi(Action1_next, Action2_next)[1] + gamma**2 * v - Q2[Action1][Action2_next])\n",
    "\n",
    "        # Update current action player 1, and determine next action based on the next state\n",
    "        Action2, Action2_next = Action2_next, get_next_state(Q2, Action1_next, actions, Epsilon)\n",
    "        \n",
    "        # Append the profits to the lists of profits\n",
    "        prof_p1_ep.append(prof_p1)\n",
    "        prof_p2_ep.append(prof_p2)\n",
    " \n",
    "        # Update epsilon\n",
    "        Epsilon = 0.1**(4*t/T)\n",
    "    \n",
    "    # Compute average profitability of the last evaluation period runs\n",
    "    average_pi1 = sum(prof_p1_ep[-L:]) / L\n",
    "    average_pi2 = sum(prof_p2_ep[-L:]) / L\n",
    "\n",
    "    # Compute average thetas of the last evaluation period runs\n",
    "    average_theta1 = sum(theta1[-L:]) / L\n",
    "    average_theta2 = sum(theta2[-L:]) / L\n",
    "\n",
    "    # Get the last 50 q's\n",
    "    last50_q1 = q1[-50:]\n",
    "    last50_q2 = q2[-50:]\n",
    "    \n",
    "    return (average_pi1, average_pi2, average_theta1, average_theta2, last50_q1, last50_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" The decision maker in this Multi-Agent Reinforcement Learning setting. \"\"\"\n",
    "\n",
    "    def __init__(self, Qs, k, gamma):\n",
    "        self.actions = None\n",
    "        self.Q = defaultdict(lambda: dict(zip(self.actions, np.zeros(len(self.actions)))))\n",
    "        self.time = 1\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def learn(state, action, next_state, profit, next_profit):\n",
    "        \"\"\" Updates the Q-function of the agent. \"\"\"\n",
    "        alpha = 0.6 - 0.5 * self.time / 500_000\n",
    "        v = max(self.Q[next_state].values())\n",
    "        self.Q[state][action] += alpha * (profit + self.gamma * next_profit\n",
    "                                          + self.gamma**2 * v - Q1[state][action])\n",
    "\n",
    "    def act():\n",
    "        \"\"\" Returns the action that should be taken according to the epsilon-greedy policy. \"\"\"\n",
    "        epsilon = 0.1 ** (4*self.time / 500_000)\n",
    "        random_action = np.random.choice(self.actions)\n",
    "        greedy_action = max(Q[state], key = Q[state].get)\n",
    "        return np.random.choice([random_action, greedy_action], p = [epsilon, 1 - epsilon])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
